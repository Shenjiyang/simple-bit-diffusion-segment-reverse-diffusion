{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import pandas as pd\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('dataset_B_05_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BITS = 4\n",
    "def decimal_to_bits(x, bits = BITS):\n",
    "    \"\"\" expects image tensor ranging from 0 to 1, outputs bit tensor ranging from -1 to 1 \"\"\"\n",
    "\n",
    "\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1)\n",
    "    mask = rearrange(mask, 'd -> 1 d ')\n",
    "    x = rearrange(x, 'b c  -> b c 1')\n",
    "\n",
    "    bits = ((x & mask) != 0).float()\n",
    "    bits = rearrange(bits, 'b c d  -> b (c d)')\n",
    "    bits = bits * 2 - 1\n",
    "    return bits\n",
    "\n",
    "def bits_to_decimal(x, bits = BITS):\n",
    "    \"\"\" expects bits from -1 to 1, outputs image tensor from 0 to 1 \"\"\"\n",
    "\n",
    "    x = (x > 0).int()\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1, dtype = torch.int32)\n",
    "\n",
    "    mask = rearrange(mask, 'd -> 1 d')\n",
    "    x = rearrange(x, 'b (c d) -> b c d', d = bits)\n",
    "    dec = reduce(x * mask, 'b c d  -> b c ', 'sum')\n",
    "    return dec.clamp(0,10) # may changed based on range of input x\n",
    "\n",
    "\n",
    "# calculate the sample value of t based on x0 and t.\n",
    "def q_x(x_0,t):\n",
    "    x_0 = decimal_to_bits(x_0)\n",
    "    noise = torch.randn_like(x_0)\n",
    "    alphas_t = alphas_bar_sqrt[t]\n",
    "    alphas_1_m_t = one_minus_alphas_bar_sqrt[t]\n",
    "    return (alphas_t * x_0 + alphas_1_m_t * noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Don't run this code repeatedly'''\n",
    "# mapping labels to 0 and 1\n",
    "df['status']=df['status'].map({'legitimate':0,'phishing':1})\n",
    "# select 1 \n",
    "df_1 = df[df['status']==1]\n",
    "# extract features\n",
    "df_2 = df_1.iloc[:,[21,86,87]]\n",
    "df_diffusion = df_2[['google_index','page_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experi_number = 5\n",
    "if experi_number == 1:\n",
    "    # control group\n",
    "    dataset = torch.Tensor(df_diffusion.values)\n",
    "elif experi_number == 2:\n",
    "    # normalize\n",
    "    scaler = StandardScaler()\n",
    "    np_scaled = scaler.fit_transform(df_diffusion)\n",
    "    dataset = torch.Tensor(np_scaled)\n",
    "elif experi_number == 3:\n",
    "    # Binary Encoder all\n",
    "    dataset = torch.Tensor(df_diffusion.values).int()\n",
    "    dataset = decimal_to_bits(dataset)\n",
    "elif experi_number == 4:\n",
    "    # Binary Encoder partly\n",
    "    dataset = torch.Tensor(df_diffusion.values).int()\n",
    "    dataset = np.concatenate([(dataset[:,0]*2-1).unsqueeze(1),decimal_to_bits(dataset[:,1].unsqueeze(1))],axis=1)\n",
    "    dataset = torch.Tensor(dataset).float()\n",
    "elif experi_number == 5:\n",
    "    # partly bianry encoder+segement\n",
    "    dataset = torch.Tensor(df_diffusion.values).int()\n",
    "    dataset = np.concatenate([(dataset[:,0]*2-1).unsqueeze(1),decimal_to_bits(dataset[:,1].unsqueeze(1))],axis=1)\n",
    "    dataset = torch.Tensor(dataset).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train parameter\n",
    "shape = dataset.shape\n",
    "batch_size = 128\n",
    "num_epoch = 3000\n",
    "if_segment = True\n",
    "\n",
    "#bata\n",
    "num_steps = 1000\n",
    "beta_min= 1.e-4\n",
    "beta_max= 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57078f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta\n",
    "betas = torch.linspace(-6,6,num_steps)\n",
    "betas = torch.sigmoid(betas)*(beta_max - beta_min) + beta_min\n",
    "\n",
    "#alpha、alpha_prod、alpha_prod_previous、alpha_bar_sqrt\n",
    "alphas = 1-betas\n",
    "alphas_prod = torch.cumprod(alphas,0)\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(),alphas_prod[:-1]],0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "\n",
    "assert alphas.shape==alphas_prod.shape==alphas_prod_p.shape==\\\n",
    "alphas_bar_sqrt.shape==one_minus_alphas_bar_log.shape\\\n",
    "==one_minus_alphas_bar_sqrt.shape\n",
    "print(\"all the same shape\",betas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583afb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self,n_steps,inout_units=5,num_units=128):\n",
    "        super(MLPDiffusion,self).__init__()\n",
    "        \n",
    "        self.linears = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(inout_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,inout_units)\n",
    "            ]\n",
    "        )\n",
    "        self.step_embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units)\n",
    "            ]\n",
    "        )\n",
    "    def forward(self,x,t):\n",
    "#         x = x_0\n",
    "        for idx,embedding_layer in enumerate(self.step_embeddings):\n",
    "            t_embedding = embedding_layer(t)\n",
    "            x = self.linears[2*idx](x)\n",
    "            x += t_embedding\n",
    "            x = self.linears[2*idx+1](x)\n",
    "            \n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss_fn(model,x_0,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,n_steps):\n",
    "    \"\"\"loss for any time t\"\"\"\n",
    "    \n",
    "    batch_size = x_0.shape[0]\n",
    "    \n",
    "    # random t, avoiding repetitive t\n",
    "    if batch_size %2 ==0:\n",
    "        t = torch.randint(0,n_steps,size=(batch_size//2,))\n",
    "        t = torch.cat([t,n_steps-1-t],dim=0)\n",
    "        t = t.unsqueeze(-1)\n",
    "    else:\n",
    "        t = torch.randint(0,n_steps,size=(batch_size//2+1,))\n",
    "        t = torch.cat([t,n_steps-1-t],dim=0)\n",
    "        t = t[:-1]\n",
    "        t = t.unsqueeze(-1)\n",
    "    \n",
    "    #coef of x0\n",
    "    a = alphas_bar_sqrt[t]\n",
    "    \n",
    "    #coef eps\n",
    "    aml = one_minus_alphas_bar_sqrt[t]\n",
    "    \n",
    "    # eps\n",
    "    e = torch.randn_like(x_0) \n",
    "    x = x_0*a+e*aml\n",
    "#     print(t.shape)\n",
    "\n",
    "    output = model(x,t.squeeze(-1))\n",
    "    \n",
    "    return (e - output).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop(model,shape,n_steps,betas,one_minus_alphas_bar_sqrt):\n",
    "    \"\"\"from x[T] restore x[T-1]、x[T-2]|...x[0]\"\"\"\n",
    "    cur_x = torch.randn(shape)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model,cur_x,i,betas,one_minus_alphas_bar_sqrt)\n",
    "        x_seq.append(cur_x)\n",
    "        \n",
    "    print('Exp 1 to 5')\n",
    "    return x_seq\n",
    "\n",
    "def p_sample(model,x,t,betas,one_minus_alphas_bar_sqrta):\n",
    "    \"\"\"from last time sample xt\"\"\"\n",
    "    t = torch.tensor([t])\n",
    "    \n",
    "    coeff = betas[t] / one_minus_alphas_bar_sqrt[t]\n",
    "    \n",
    "    eps_theta = model(x,t)\n",
    "    \n",
    "    # Segment inverse diffusion\n",
    "    if t==0 and if_segment==True:\n",
    "        t = torch.tensor([t])\n",
    "        x_0 = (x - betas[t].sqrt()*eps_theta)/(1-betas[t]).sqrt() \n",
    "        sample = x_0\n",
    "    else:\n",
    "        mean = (1/(1-betas[t]).sqrt())*(x-(coeff*eps_theta))\n",
    "\n",
    "        z = torch.randn_like(x)\n",
    "        sigma_t = betas[t].sqrt()\n",
    "\n",
    "        sample = mean + sigma_t * z\n",
    "    \n",
    "    return (sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1edce",
   "metadata": {},
   "source": [
    "# 1 Control Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa877191",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "plt.rc('text',color='blue')\n",
    "\n",
    "model = MLPDiffusion(n_steps=num_steps,inout_units=shape[1])  #num_steps=1000, inout\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    for idx,batch_x in enumerate(dataloader):\n",
    "        loss = diffusion_loss_fn(model,batch_x,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.)\n",
    "        optimizer.step()\n",
    "        \n",
    "    if(t%100==0):\n",
    "        print(loss)\n",
    "        x_seq = p_sample_loop(model,shape,num_steps,betas,one_minus_alphas_bar_sqrt)\n",
    "        \n",
    "        fig,axs = plt.subplots(1,10,figsize=(28,3))\n",
    "        for i in range(1,11):\n",
    "            cur_x = x_seq[i*100].detach()\n",
    "            axs[i-1].scatter(cur_x[:,-2],cur_x[:,-1],color='red',edgecolor='white');\n",
    "            axs[i-1].set_axis_off();\n",
    "            axs[i-1].set_title('$q(\\mathbf{x}_{'+str(i*10)+'})$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a76aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample x0\n",
    "generate = p_sample_loop(model,shape,num_steps,betas,one_minus_alphas_bar_sqrt)\n",
    "x_0 = generate[1000].detach()\n",
    "\n",
    "# add labels for generative data\n",
    "n_1 = np.ones((5715,1))\n",
    "X_gen = np.concatenate([x_0,n_1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d554da9",
   "metadata": {},
   "source": [
    "# 2 Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457da983",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "print('Training model...')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "plt.rc('text',color='blue')\n",
    "\n",
    "model = MLPDiffusion(n_steps=num_steps,inout_units=shape[1])  #num_steps=1000, inout\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    for idx,batch_x in enumerate(dataloader):\n",
    "        loss = diffusion_loss_fn(model,batch_x,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.)\n",
    "        optimizer.step()\n",
    "        \n",
    "    if(t%100==0):\n",
    "        print(loss)\n",
    "        x_seq = p_sample_loop(model,shape,num_steps,betas,one_minus_alphas_bar_sqrt)\n",
    "        \n",
    "        fig,axs = plt.subplots(1,10,figsize=(28,3))\n",
    "        for i in range(1,11):\n",
    "            cur_x = x_seq[i*100].detach()\n",
    "            axs[i-1].scatter(cur_x[:,-2],cur_x[:,-1],color='red',edgecolor='white');\n",
    "            axs[i-1].set_axis_off();\n",
    "            axs[i-1].set_title('$q(\\mathbf{x}_{'+str(i*10)+'})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df768b6",
   "metadata": {},
   "source": [
    "# 3 BE all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "print('Training model...')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "plt.rc('text',color='blue')\n",
    "\n",
    "model = MLPDiffusion(n_steps=num_steps,inout_units=shape[1])  #num_steps=1000, inout\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    for idx,batch_x in enumerate(dataloader):\n",
    "        loss = diffusion_loss_fn(model,batch_x,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.)\n",
    "        optimizer.step()\n",
    "        \n",
    "    if(t%100==0):\n",
    "        print(loss)\n",
    "        x_seq = p_sample_loop(model,shape,num_steps,betas,one_minus_alphas_bar_sqrt)\n",
    "        \n",
    "        fig,axs = plt.subplots(1,10,figsize=(28,3))\n",
    "        for i in range(1,11):\n",
    "            cur_x = x_seq[i*100].detach()\n",
    "            axs[i-1].scatter(cur_x[:,-2],cur_x[:,-1],color='red',edgecolor='white');\n",
    "            axs[i-1].set_axis_off();\n",
    "            axs[i-1].set_title('$q(\\mathbf{x}_{'+str(i*10)+'})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4347ca",
   "metadata": {},
   "source": [
    "# 4 BE partly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "print('Training model...')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "plt.rc('text',color='blue')\n",
    "\n",
    "model = MLPDiffusion(n_steps=num_steps,inout_units=shape[1])  #num_steps=1000, inout\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    for idx,batch_x in enumerate(dataloader):\n",
    "        loss = diffusion_loss_fn(model,batch_x,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.)\n",
    "        optimizer.step()\n",
    "        \n",
    "    if(t%500==0):\n",
    "        print(loss)\n",
    "        x_seq = p_sample_loop(model,shape,num_steps,betas,one_minus_alphas_bar_sqrt)\n",
    "        \n",
    "        fig,axs = plt.subplots(1,10,figsize=(28,3))\n",
    "        for i in range(1,11):\n",
    "            cur_x = x_seq[i*100].detach()\n",
    "            axs[i-1].scatter(cur_x[:,-2],cur_x[:,-1],color='red',edgecolor='white');\n",
    "            axs[i-1].set_axis_off();\n",
    "            axs[i-1].set_title('$q(\\mathbf{x}_{'+str(i*10)+'})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7c9a4",
   "metadata": {},
   "source": [
    "# 5 BE partly + Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fa120",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "print('Training model...')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "plt.rc('text',color='blue')\n",
    "\n",
    "model = MLPDiffusion(n_steps=num_steps,inout_units=shape[1])  #num_steps=1000, inout\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    for idx,batch_x in enumerate(dataloader):\n",
    "        loss = diffusion_loss_fn(model,batch_x,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.)\n",
    "        optimizer.step()\n",
    "    \n",
    "    if(t%500==0):\n",
    "        print(loss)\n",
    "        x_seq = p_sample_loop(model,shape,num_steps,betas,one_minus_alphas_bar_sqrt)\n",
    "        \n",
    "        fig,axs = plt.subplots(1,10,figsize=(28,3))\n",
    "        for i in range(1,11):\n",
    "            cur_x = x_seq[i*100].detach()\n",
    "            axs[i-1].scatter(cur_x[:,-2],cur_x[:,-1],color='red',edgecolor='white');\n",
    "            axs[i-1].set_axis_off();\n",
    "            axs[i-1].set_title('$q(\\mathbf{x}_{'+str(i*10)+'})$')    \n",
    "#     if(t%200==0):\n",
    "#         print(loss)\n",
    "#         x_seq = p_sample_loop(model,shape,num_steps,betas,one_minus_alphas_bar_sqrt)\n",
    "        \n",
    "#         fig,axs = plt.subplots(1,2,figsize=(6,3))\n",
    "#         for i in range(1,3):\n",
    "#             cur_x = x_seq[i+998].detach()\n",
    "#             axs[i-1].scatter(cur_x[:,-2],cur_x[:,-1],color='red',edgecolor='white');\n",
    "#             axs[i-1].set_axis_off();\n",
    "#             axs[i-1].set_title('$q(\\mathbf{x}_{'+str(i*10)+'})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727bdf4",
   "metadata": {},
   "source": [
    "# 6 Limited data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,21):\n",
    "\n",
    "    num_samples = 20\n",
    "    #randomly select\n",
    "    indices = torch.randperm(dataset.size(0))[:num_samples*j]\n",
    "    sampled_dataset = dataset[indices]\n",
    "\n",
    "    model = MLPDiffusion(n_steps=num_steps,inout_units=shape[1])  #num_steps=1000, inout\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    \n",
    "    for t in range(num_epoch):\n",
    "   \n",
    "        loss = diffusion_loss_fn(model,sampled_dataset,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.)\n",
    "        optimizer.step()\n",
    "\n",
    "        if(t%500==0):\n",
    "            print(loss)\n",
    "\n",
    "    print(j)   \n",
    "\n",
    "    generate = p_sample_loop(model,shape,num_steps,betas,one_minus_alphas_bar_sqrt)\n",
    "    x_0 = generate[1000].detach()\n",
    "\n",
    "    decimal_x = np.concatenate([(x_0[:,[0]]>0).int(),bits_to_decimal(x_0[:,1:])],axis=1)\n",
    "\n",
    "    n_1 = np.ones((5715,1))\n",
    "    X_gen = np.concatenate([decimal_x,n_1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6dedbc",
   "metadata": {},
   "source": [
    "# 7 predict x0 instead of e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss_fn(model,x_0,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,n_steps):\n",
    "    \"\"\"loss\"\"\"\n",
    "    print('exp 7')\n",
    "    batch_size = x_0.shape[0]\n",
    "    \n",
    "    if batch_size %2 ==0:\n",
    "        t = torch.randint(0,n_steps,size=(batch_size//2,))\n",
    "        t = torch.cat([t,n_steps-1-t],dim=0)\n",
    "        t = t.unsqueeze(-1)\n",
    "    else:\n",
    "        t = torch.randint(0,n_steps,size=(batch_size//2+1,))\n",
    "        t = torch.cat([t,n_steps-1-t],dim=0)\n",
    "        t = t[:-1]\n",
    "        t = t.unsqueeze(-1)\n",
    "    \n",
    "    a = alphas_bar_sqrt[t]\n",
    "    aml = one_minus_alphas_bar_sqrt[t]\n",
    "    e = torch.randn_like(x_0)\n",
    "    \n",
    "    x = x_0*a+e*aml\n",
    "    output = model(x,t.squeeze(-1))\n",
    "    \n",
    "\n",
    "    return (x_0 - output).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop(model,shape,n_steps,betas,one_minus_alphas_bar_sqrt):\n",
    "    cur_x = torch.randn(shape)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model,cur_x,i,betas,one_minus_alphas_bar_sqrt)\n",
    "        x_seq.append(cur_x)\n",
    "        \n",
    "    print('exp 7')\n",
    "    return x_seq\n",
    "\n",
    "def p_sample(model,x,t,betas,one_minus_alphas_bar_sqrta):\n",
    "    t = torch.tensor([t])\n",
    "    \n",
    "    coeff = betas[t] / one_minus_alphas_bar_sqrt[t]\n",
    "    \n",
    "    x_0 = model(x,t)\n",
    "    \n",
    "    pred_noise = (x - alphas_bar_sqrt[t] * x_0) / one_minus_alphas_bar_sqrt[t].clamp(min = 1e-8)\n",
    "\n",
    "            # calculate x next\n",
    "\n",
    "    img = x_start * alpha_next + pred_noise * sigma_next\n",
    "    \n",
    "    return (sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1b945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_X_gen = 4000\n",
    "num_X_pos = 0\n",
    "acc_lst = [] #实验组\n",
    "pre_lst = []\n",
    "rec_lst = []\n",
    "f1_lst = []\n",
    "\n",
    "acc_duizhao_lst = [] #对照组\n",
    "pre_duizhao_lst = []\n",
    "rec_duizhao_lst = []\n",
    "f1_duizhao_lst = []\n",
    "\n",
    "acc_cossen_lst =[0]#损失敏感组\n",
    "pre_cossen_lst = [0]\n",
    "rec_cossen_lst = [0]\n",
    "f1_cossen_lst = [0]\n",
    "\n",
    "acc_upsam_lst = [0] #上采样组\n",
    "pre_upsam_lst = [0]\n",
    "rec_upsam_lst = [0]\n",
    "f1_upsam_lst = [0]\n",
    "\n",
    "acc_smote_lst = [0] #smote组\n",
    "pre_smote_lst = [0]\n",
    "rec_smote_lst = [0]\n",
    "f1_smote_lst = [0]\n",
    "\n",
    "\n",
    "X_neg = np.array(df[['google_index','page_rank','status']][df['status']==0])\n",
    "X_pos = np.array(df[['google_index','page_rank','status']][df['status']==1])\n",
    "\n",
    "for i in range(201):\n",
    "    np.random.shuffle(X_neg)  \n",
    "    np.random.shuffle(X_pos)\n",
    "    np.random.shuffle(X_gen)\n",
    "    \n",
    "    X_y_train = np.concatenate([X_gen[:num_X_gen-20*i,:],X_pos[:20*i,:],X_neg[:4000,:]],axis=0)\n",
    "    np.random.shuffle(X_y_train)\n",
    "    X_y_test = np.concatenate([X_pos[4000:5000,:],X_neg[4000:5000,:]],axis=0)\n",
    "    np.random.shuffle(X_y_test)\n",
    "    \n",
    "    clf =xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    clf.fit(X_y_train[:,:-1],X_y_train[:,-1])\n",
    "\n",
    "    y_tes = X_y_test[:,-1]\n",
    "    y_pre = clf.predict(X_y_test[:,:-1])\n",
    "\n",
    "    accuracy = accuracy_score(y_tes, y_pre)\n",
    "    precision = precision_score(y_tes, y_pre)\n",
    "    recall = recall_score(y_tes, y_pre)\n",
    "    f1 = f1_score(y_tes, y_pre)\n",
    "#     print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "    acc_lst.append(accuracy)\n",
    "    pre_lst.append(precision)\n",
    "    rec_lst.append(recall)\n",
    "    f1_lst.append(f1)\n",
    "    \n",
    "    if i == 5:\n",
    "        y_pred_prob = clf.predict_proba(X_y_test[:,:-1])[:,1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_tes, y_pred_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr,lw=2, label='SRDM (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Classifier ROC')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "#         plt.show()\n",
    "        \n",
    "for i in range(201):\n",
    "    np.random.shuffle(X_neg)  \n",
    "    np.random.shuffle(X_pos)\n",
    "    np.random.shuffle(X_gen)\n",
    "    \n",
    "    X_y_train = np.concatenate([X_pos[:20*i,:],X_neg[:4000,:]],axis=0)\n",
    "    np.random.shuffle(X_y_train)\n",
    "    X_y_test = np.concatenate([X_pos[4000:5000,:],X_neg[4000:5000,:]],axis=0)\n",
    "    np.random.shuffle(X_y_test)\n",
    "    \n",
    "    clf =xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    clf.fit(X_y_train[:,:-1],X_y_train[:,-1])\n",
    "    y_tes = X_y_test[:,-1]\n",
    "    y_pre = clf.predict(X_y_test[:,:-1])\n",
    "\n",
    "    accuracy = accuracy_score(y_tes, y_pre)\n",
    "    precision = precision_score(y_tes, y_pre)\n",
    "    recall = recall_score(y_tes, y_pre)\n",
    "    f1 = f1_score(y_tes, y_pre)\n",
    "    \n",
    "    acc_duizhao_lst.append(accuracy)\n",
    "    pre_duizhao_lst.append(precision)\n",
    "    rec_duizhao_lst.append(recall)\n",
    "    f1_duizhao_lst.append(f1)\n",
    "    \n",
    "    if i == 5:\n",
    "        y_pred_prob =clf.predict_proba(X_y_test[:,:-1])[:,1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_tes, y_pred_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#         plt.figure()\n",
    "        plt.plot(fpr, tpr, lw=2, label='control group(area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1],  lw=2, linestyle='--')\n",
    "\n",
    "\n",
    "    if i > 0:\n",
    "        #cost sens\n",
    "        scale_pos_weight = 4000 / (40*i)\n",
    "        clf = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight,use_label_encoder=False, eval_metric='logloss')\n",
    "        clf.fit(X_y_train[:,:-1],X_y_train[:,-1])\n",
    "        y_tes = X_y_test[:,-1]\n",
    "        y_pre = clf.predict(X_y_test[:,:-1])\n",
    "\n",
    "        accuracy = accuracy_score(y_tes, y_pre)\n",
    "        precision = precision_score(y_tes, y_pre)\n",
    "        recall = recall_score(y_tes, y_pre)\n",
    "        f1 = f1_score(y_tes, y_pre)\n",
    "        \n",
    "        acc_cossen_lst.append(accuracy)\n",
    "        pre_cossen_lst.append(precision)\n",
    "        rec_cossen_lst.append(recall)\n",
    "        f1_cossen_lst.append(f1)\n",
    "        \n",
    "        if i == 5:\n",
    "            y_pred_prob =clf.predict_proba(X_y_test[:,:-1])[:,1]\n",
    "            fpr, tpr, thresholds = roc_curve(y_tes, y_pred_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "#             plt.figure()\n",
    "            plt.plot(fpr, tpr, lw=2, label='cost sensitive(area = %0.2f)' % roc_auc)\n",
    "            plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "#             plt.show()\n",
    "        \n",
    "        # oversample\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_y_train[:,:-1], X_y_train[:,-1])\n",
    "\n",
    "        clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        clf.fit(X_resampled, y_resampled)\n",
    "        y_tes = X_y_test[:,-1]\n",
    "        y_pre = clf.predict(X_y_test[:,:-1])\n",
    "\n",
    "        accuracy = accuracy_score(y_tes, y_pre)\n",
    "        precision = precision_score(y_tes, y_pre)\n",
    "        recall = recall_score(y_tes, y_pre)\n",
    "        f1 = f1_score(y_tes, y_pre)\n",
    "        \n",
    "        acc_upsam_lst.append(accuracy)\n",
    "        pre_upsam_lst.append(precision)\n",
    "        rec_upsam_lst.append(recall)\n",
    "        f1_upsam_lst.append(f1)\n",
    "        \n",
    "        if i == 5:\n",
    "            y_pred_prob =clf.predict_proba(X_y_test[:,:-1])[:,1]\n",
    "            fpr, tpr, thresholds = roc_curve(y_tes, y_pred_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr,  lw=2, label='upper sampling (area = %0.2f)' % roc_auc)\n",
    "            plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "   \n",
    "\n",
    "    \n",
    "        #SMOTE\n",
    "        sm = SMOTE(random_state=42)\n",
    "\n",
    "        X_resampled, y_resampled = sm.fit_resample(X_y_train[:,:-1], X_y_train[:,-1])\n",
    "        clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "        clf.fit(X_resampled, y_resampled)\n",
    "        y_tes = X_y_test[:,-1]\n",
    "        y_pre = clf.predict(X_y_test[:,:-1])\n",
    "\n",
    "        accuracy = accuracy_score(y_tes, y_pre)\n",
    "        precision = precision_score(y_tes, y_pre)\n",
    "        recall = recall_score(y_tes, y_pre)\n",
    "        f1 = f1_score(y_tes, y_pre)\n",
    "        \n",
    "        acc_smote_lst.append(accuracy)\n",
    "        pre_smote_lst.append(precision)\n",
    "        rec_smote_lst.append(recall)\n",
    "        f1_smote_lst.append(f1)\n",
    "        \n",
    "        if i == 5:\n",
    "            y_pred_prob =clf.predict_proba(X_y_test[:,:-1])[:,1]\n",
    "            fpr, tpr, thresholds = roc_curve(y_tes, y_pred_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, lw=2, label='smote (area = %0.2f)' % roc_auc)\n",
    "            plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185634af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
